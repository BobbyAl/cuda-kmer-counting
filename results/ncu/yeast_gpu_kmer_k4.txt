Reading sequence from ../data/ncbi_dataset_saccharomyces_cerevisiae/ncbi_dataset/data/GCF_000146045.2/GCF_000146045.2_R64_genomic.fna...
Sequence length: 12158327 bases
==PROF== Connected to process 205988 (/home/bobby/Documents/GitHub/cuda-kmer-counting/src/gpu_kmer)
Processing sequence in chunks of 1000000 bases
==PROF== Profiling "count_kmers_kernel" - 0: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 1: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 2: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 3: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 4: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 5: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 6: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 7: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 8: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 9: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 10: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 11: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 12: 0%....50%....100% - 31 passes
Found 256 unique 4-mers
GPU time: 36.3797 seconds
Top 10 most frequent k-mers: 
TTTT: 685200
AAAA: 685001
AAAT: 674236
ATTT: 672554
AATT: 627775
CAAA: 584379
AATA: 582454
TATT: 582228
TTTG: 581551
ATAT: 574277
==PROF== Disconnected from process 205988
[205988] gpu_kmer@127.0.0.1
  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 13, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.00
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    5,577,312
    Memory Throughput                 %         8.60
    DRAM Throughput                   %         1.60
    Duration                         ms         4.04
    L1/TEX Cache Throughput           %        12.84
    L2 Cache Throughput               %         8.60
    SM Active Cycles              cycle 5,562,733.83
    Compute (SM) Throughput           %        10.83
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         1.57
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.85
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.85
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.43%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         5.39
    Mem Busy                    %         6.42
    Max Bandwidth               %         8.60
    L1/TEX Hit Rate             %        20.87
    L2 Hit Rate                 %        93.27
    Mem Pipes Busy              %         2.58
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.778%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.86
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.14
    Active Warps Per Scheduler          warp         7.02
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.14%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.02 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.64
    Warp Cycles Per Executed Instruction           cycle        66.34
    Avg. Active Threads Per Warp                                19.93
    Avg. Not Predicated Off Threads Per Warp                    18.32
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62.37%                                                                                          
          On average, each warp of this workload spends 40.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.4% of the total average of 64.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.63%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   588,024.72
    Executed Instructions                           inst   70,562,967
    Avg. Issued Instructions Per Scheduler          inst   603,476.16
    Issued Instructions                             inst   72,417,139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 13,312
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       3,407,872
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              110.93
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.78
    Achieved Active Warps Per SM           warp        28.09
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.22%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   453,800.67
    Total DRAM Elapsed Cycles        cycle  169,707,520
    Average L1 Active Cycles         cycle 5,562,733.83
    Total L1 Elapsed Cycles          cycle  167,107,440
    Average L2 Active Cycles         cycle 1,569,298.92
    Total L2 Elapsed Cycles          cycle  126,528,312
    Average SM Active Cycles         cycle 5,562,733.83
    Total SM Elapsed Cycles          cycle  167,107,440
    Average SMSP Active Cycles       cycle 5,558,890.15
    Total SMSP Elapsed Cycles        cycle  668,429,760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.89%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 70.18% above the average, while the minimum instance value is 24.59% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst   22,889,601
    Branch Efficiency                   %        62.56
    Avg. Divergent Branches                  39,677.88
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 20.43%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 6223873 excessive sectors (69% of the     
          total 9068741 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 12, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.00
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    5,489,731
    Memory Throughput                 %         8.51
    DRAM Throughput                   %         0.98
    Duration                         ms         3.98
    L1/TEX Cache Throughput           %        12.84
    L2 Cache Throughput               %         8.51
    SM Active Cycles              cycle 5,480,993.80
    Compute (SM) Throughput           %        10.84
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         1.57
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.85
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.85
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.42%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.28
    Mem Busy                    %         6.22
    Max Bandwidth               %         8.51
    L1/TEX Hit Rate             %        20.88
    L2 Hit Rate                 %        96.49
    Mem Pipes Busy              %         2.58
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.781%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.84
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.16
    Active Warps Per Scheduler          warp         7.02
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.16%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.02 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.78
    Warp Cycles Per Executed Instruction           cycle        66.49
    Avg. Active Threads Per Warp                                19.89
    Avg. Not Predicated Off Threads Per Warp                    18.29
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62.42%                                                                                          
          On average, each warp of this workload spends 40.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.4% of the total average of 64.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.643%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   579,655.12
    Executed Instructions                           inst   69,558,614
    Avg. Issued Instructions Per Scheduler          inst   594,952.15
    Issued Instructions                             inst   71,394,258
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 12,288
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       3,145,728
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              102.40
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.76
    Achieved Active Warps Per SM           warp        28.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.24%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      271,978
    Total DRAM Elapsed Cycles        cycle  166,978,560
    Average L1 Active Cycles         cycle 5,480,993.80
    Total L1 Elapsed Cycles          cycle  164,692,740
    Average L2 Active Cycles         cycle 1,705,391.25
    Total L2 Elapsed Cycles          cycle  124,562,328
    Average SM Active Cycles         cycle 5,480,993.80
    Total SM Elapsed Cycles          cycle  164,692,740
    Average SMSP Active Cycles       cycle 5,486,667.19
    Total SMSP Elapsed Cycles        cycle  658,770,960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.07%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 67.16% above the average, while the minimum instance value is 21.22% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst   22,590,677
    Branch Efficiency                   %        62.50
    Avg. Divergent Branches                  39,223.47
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 22.57%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 6153521 excessive sectors (69% of the     
          total 8959839 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 11, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.00
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    5,039,522
    Memory Throughput                 %         8.62
    DRAM Throughput                   %         0.97
    Duration                         ms         3.65
    L1/TEX Cache Throughput           %        12.84
    L2 Cache Throughput               %         8.62
    SM Active Cycles              cycle 5,032,041.60
    Compute (SM) Throughput           %        10.84
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         1.57
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.84
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.84
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.43%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.27
    Mem Busy                    %         6.45
    Max Bandwidth               %         8.62
    L1/TEX Hit Rate             %        20.88
    L2 Hit Rate                 %        92.95
    Mem Pipes Busy              %         2.58
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.781%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.85
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.15
    Active Warps Per Scheduler          warp         7.02
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.15%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.02 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.68
    Warp Cycles Per Executed Instruction           cycle        66.39
    Avg. Active Threads Per Warp                                19.90
    Avg. Not Predicated Off Threads Per Warp                    18.29
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62.56%                                                                                          
          On average, each warp of this workload spends 40.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.6% of the total average of 64.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.641%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   531,567.16
    Executed Instructions                           inst   63,788,059
    Avg. Issued Instructions Per Scheduler          inst   545,592.22
    Issued Instructions                             inst   65,471,067
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 11,264
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       2,883,584
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               93.87
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.74
    Achieved Active Warps Per SM           warp        28.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.26%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      249,108
    Total DRAM Elapsed Cycles        cycle  153,494,528
    Average L1 Active Cycles         cycle 5,032,041.60
    Total L1 Elapsed Cycles          cycle  151,053,540
    Average L2 Active Cycles         cycle 1,422,927.83
    Total L2 Elapsed Cycles          cycle  114,312,504
    Average SM Active Cycles         cycle 5,032,041.60
    Total SM Elapsed Cycles          cycle  151,053,540
    Average SMSP Active Cycles       cycle 5,027,421.38
    Total SMSP Elapsed Cycles        cycle  604,214,160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.94%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 70.10% above the average, while the minimum instance value is 25.00% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst   20,716,722
    Branch Efficiency                   %        62.50
    Avg. Divergent Branches                  35,968.58
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 20.52%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 5645022 excessive sectors (69% of the     
          total 8218997 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.01
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    4,580,875
    Memory Throughput                 %         8.50
    DRAM Throughput                   %         0.98
    Duration                         ms         3.32
    L1/TEX Cache Throughput           %        12.84
    L2 Cache Throughput               %         8.50
    SM Active Cycles              cycle 4,573,327.43
    Compute (SM) Throughput           %        10.84
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         1.57
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.84
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.84
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.43%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.29
    Mem Busy                    %         6.48
    Max Bandwidth               %         8.50
    L1/TEX Hit Rate             %        20.88
    L2 Hit Rate                 %        92.61
    Mem Pipes Busy              %         2.58
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.781%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.86
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.14
    Active Warps Per Scheduler          warp         7.01
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.14%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.01 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.60
    Warp Cycles Per Executed Instruction           cycle        66.31
    Avg. Active Threads Per Warp                                19.90
    Avg. Not Predicated Off Threads Per Warp                    18.29
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62.61%                                                                                          
          On average, each warp of this workload spends 40.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.6% of the total average of 64.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.643%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   483,126.25
    Executed Instructions                           inst   57,975,150
    Avg. Issued Instructions Per Scheduler          inst   495,879.88
    Issued Instructions                             inst   59,505,585
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 10,240
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       2,621,440
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               85.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.67
    Achieved Active Warps Per SM           warp        28.06
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.33%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   227,503.33
    Total DRAM Elapsed Cycles        cycle  139,591,680
    Average L1 Active Cycles         cycle 4,573,327.43
    Total L1 Elapsed Cycles          cycle  137,264,530
    Average L2 Active Cycles         cycle 1,294,569.21
    Total L2 Elapsed Cycles          cycle  103,884,528
    Average SM Active Cycles         cycle 4,573,327.43
    Total SM Elapsed Cycles          cycle  137,264,530
    Average SMSP Active Cycles       cycle 4,567,556.23
    Total SMSP Elapsed Cycles        cycle  549,058,120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.96%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 70.07% above the average, while the minimum instance value is 24.63% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst   18,828,692
    Branch Efficiency                   %        62.50
    Avg. Divergent Branches                  32,689.89
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 20.54%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 5129827 excessive sectors (69% of the     
          total 7469025 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 9, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.01
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    4,121,712
    Memory Throughput                 %         8.50
    DRAM Throughput                   %         0.99
    Duration                         ms         2.99
    L1/TEX Cache Throughput           %        12.82
    L2 Cache Throughput               %         8.50
    SM Active Cycles              cycle 4,110,012.77
    Compute (SM) Throughput           %        10.83
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         1.57
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.85
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.85
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.42%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.32
    Mem Busy                    %         6.51
    Max Bandwidth               %         8.50
    L1/TEX Hit Rate             %        20.88
    L2 Hit Rate                 %        92.18
    Mem Pipes Busy              %         2.58
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.773%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.85
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.15
    Active Warps Per Scheduler          warp         7.02
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.15%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.02 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.66
    Warp Cycles Per Executed Instruction           cycle        66.37
    Avg. Active Threads Per Warp                                19.89
    Avg. Not Predicated Off Threads Per Warp                    18.29
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62.47%                                                                                          
          On average, each warp of this workload spends 40.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.5% of the total average of 64.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.64%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   434,610.88
    Executed Instructions                           inst   52,153,306
    Avg. Issued Instructions Per Scheduler          inst   446,096.98
    Issued Instructions                             inst   53,531,638
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  9,216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       2,359,296
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               76.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.71
    Achieved Active Warps Per SM           warp        28.07
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.29%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      206,792
    Total DRAM Elapsed Cycles        cycle  125,580,288
    Average L1 Active Cycles         cycle 4,110,012.77
    Total L1 Elapsed Cycles          cycle  123,601,910
    Average L2 Active Cycles         cycle 1,159,324.42
    Total L2 Elapsed Cycles          cycle   93,449,760
    Average SM Active Cycles         cycle 4,110,012.77
    Total SM Elapsed Cycles          cycle  123,601,910
    Average SMSP Active Cycles       cycle 4,110,456.38
    Total SMSP Elapsed Cycles        cycle  494,407,640
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.9%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 70.20% above the average, while the minimum instance value is 24.82% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst   16,937,733
    Branch Efficiency                   %        62.49
    Avg. Divergent Branches                  29,410.74
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 20.45%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4612077 excessive sectors (69% of the     
          total 6715884 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 8, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    3,665,826
    Memory Throughput                 %         8.50
    DRAM Throughput                   %         1.00
    Duration                         ms         2.66
    L1/TEX Cache Throughput           %        12.82
    L2 Cache Throughput               %         8.50
    SM Active Cycles              cycle 3,656,711.37
    Compute (SM) Throughput           %        10.83
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         1.57
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.85
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.85
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.42%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.36
    Mem Busy                    %         6.22
    Max Bandwidth               %         8.50
    L1/TEX Hit Rate             %        20.88
    L2 Hit Rate                 %        96.50
    Mem Pipes Busy              %         2.58
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.775%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.84
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.16
    Active Warps Per Scheduler          warp         7.00
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.16%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.00 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.61
    Warp Cycles Per Executed Instruction           cycle        66.31
    Avg. Active Threads Per Warp                                19.89
    Avg. Not Predicated Off Threads Per Warp                    18.29
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62.57%                                                                                          
          On average, each warp of this workload spends 40.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.6% of the total average of 64.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.64%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   386,470.62
    Executed Instructions                           inst   46,376,475
    Avg. Issued Instructions Per Scheduler          inst   396,690.12
    Issued Instructions                             inst   47,602,815
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  8,192
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       2,097,152
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               68.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.84
    Achieved Active Warps Per SM           warp        28.11
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.16%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   186,187.33
    Total DRAM Elapsed Cycles        cycle  111,432,704
    Average L1 Active Cycles         cycle 3,656,711.37
    Total L1 Elapsed Cycles          cycle  109,889,180
    Average L2 Active Cycles         cycle 1,186,210.88
    Total L2 Elapsed Cycles          cycle   83,131,128
    Average SM Active Cycles         cycle 3,656,711.37
    Total SM Elapsed Cycles          cycle  109,889,180
    Average SMSP Active Cycles       cycle 3,659,378.36
    Total SMSP Elapsed Cycles        cycle  439,556,720
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.52%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 65.77% above the average, while the minimum instance value is 20.27% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst   15,062,194
    Branch Efficiency                   %        62.49
    Avg. Divergent Branches                  26,155.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 23.52%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4102249 excessive sectors (69% of the     
          total 5973232 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 7, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.00
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    3,208,438
    Memory Throughput                 %         8.50
    DRAM Throughput                   %         1.02
    Duration                         ms         2.33
    L1/TEX Cache Throughput           %        12.83
    L2 Cache Throughput               %         8.50
    SM Active Cycles              cycle 3,198,070.17
    Compute (SM) Throughput           %        10.83
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       786.43
    Dropped Samples                sample           11
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.86
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.86
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.42%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.41
    Mem Busy                    %         6.27
    Max Bandwidth               %         8.50
    L1/TEX Hit Rate             %        20.88
    L2 Hit Rate                 %        95.70
    Mem Pipes Busy              %         2.58
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.776%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.86
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.14
    Active Warps Per Scheduler          warp         7.03
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.14%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.03 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.72
    Warp Cycles Per Executed Instruction           cycle        66.44
    Avg. Active Threads Per Warp                                19.89
    Avg. Not Predicated Off Threads Per Warp                    18.29
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62.35%                                                                                          
          On average, each warp of this workload spends 40.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.4% of the total average of 64.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.642%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   338,285.22
    Executed Instructions                           inst   40,594,226
    Avg. Issued Instructions Per Scheduler          inst   347,238.42
    Issued Instructions                             inst   41,668,611
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  7,168
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       1,835,008
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               59.73
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.78
    Achieved Active Warps Per SM           warp        28.09
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.22%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   165,144.67
    Total DRAM Elapsed Cycles        cycle   97,601,536
    Average L1 Active Cycles         cycle 3,198,070.17
    Total L1 Elapsed Cycles          cycle   96,171,180
    Average L2 Active Cycles         cycle   903,360.46
    Total L2 Elapsed Cycles          cycle   72,764,016
    Average SM Active Cycles         cycle 3,198,070.17
    Total SM Elapsed Cycles          cycle   96,171,180
    Average SMSP Active Cycles       cycle 3,198,771.46
    Total SMSP Elapsed Cycles        cycle  384,684,720
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.91%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 70.17% above the average, while the minimum instance value is 25.15% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst   13,184,455
    Branch Efficiency                   %        62.49
    Avg. Divergent Branches                  22,898.07
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 20.46%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 3590518 excessive sectors (69% of the     
          total 5228260 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 6, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    2,746,783
    Memory Throughput                 %         8.50
    DRAM Throughput                   %         1.04
    Duration                         ms         1.99
    L1/TEX Cache Throughput           %        12.83
    L2 Cache Throughput               %         8.50
    SM Active Cycles              cycle 2,746,508.80
    Compute (SM) Throughput           %        10.84
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       786.43
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.83
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.83
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.43%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.48
    Mem Busy                    %         6.22
    Max Bandwidth               %         8.50
    L1/TEX Hit Rate             %        20.89
    L2 Hit Rate                 %        96.56
    Mem Pipes Busy              %         2.58
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.778%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.87
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.13
    Active Warps Per Scheduler          warp         7.01
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.13%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.01 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.56
    Warp Cycles Per Executed Instruction           cycle        66.27
    Avg. Active Threads Per Warp                                19.88
    Avg. Not Predicated Off Threads Per Warp                    18.28
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62.33%                                                                                          
          On average, each warp of this workload spends 40.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.3% of the total average of 64.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.65%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   289,798.38
    Executed Instructions                           inst   34,775,806
    Avg. Issued Instructions Per Scheduler          inst   297,483.44
    Issued Instructions                             inst   35,698,013
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  6,144
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       1,572,864
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               51.20
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.37
    Achieved Active Warps Per SM           warp        27.96
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.63%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   144,222.67
    Total DRAM Elapsed Cycles        cycle   83,502,080
    Average L1 Active Cycles         cycle 2,746,508.80
    Total L1 Elapsed Cycles          cycle   82,301,320
    Average L2 Active Cycles         cycle   773,872.58
    Total L2 Elapsed Cycles          cycle   62,276,856
    Average SM Active Cycles         cycle 2,746,508.80
    Total SM Elapsed Cycles          cycle   82,301,320
    Average SMSP Active Cycles       cycle 2,737,963.09
    Total SMSP Elapsed Cycles        cycle  329,205,280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.92%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 70.14% above the average, while the minimum instance value is 24.68% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst   11,294,585
    Branch Efficiency                   %        62.47
    Avg. Divergent Branches                  19,620.09
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 20.48%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 3072300 excessive sectors (69% of the     
          total 4474723 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 5, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.00
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle    2,289,788
    Memory Throughput                 %         8.50
    DRAM Throughput                   %         1.05
    Duration                         ms         1.68
    L1/TEX Cache Throughput           %        12.77
    L2 Cache Throughput               %         8.50
    SM Active Cycles              cycle 2,287,159.70
    Compute (SM) Throughput           %        10.81
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       786.43
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.83
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.83
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.44%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.54
    Mem Busy                    %         6.22
    Max Bandwidth               %         8.50
    L1/TEX Hit Rate             %        20.90
    L2 Hit Rate                 %        96.45
    Mem Pipes Busy              %         2.57
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.752%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.86
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.14
    Active Warps Per Scheduler          warp         7.02
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.14%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.02 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.59
    Warp Cycles Per Executed Instruction           cycle        66.31
    Avg. Active Threads Per Warp                                19.87
    Avg. Not Predicated Off Threads Per Warp                    18.27
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62.41%                                                                                          
          On average, each warp of this workload spends 40.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.4% of the total average of 64.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.637%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   241,246.27
    Executed Instructions                           inst   28,949,552
    Avg. Issued Instructions Per Scheduler          inst   247,658.48
    Issued Instructions                             inst   29,719,018
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  5,120
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       1,310,720
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               42.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.40
    Achieved Active Warps Per SM           warp        27.97
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.6%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   123,552.67
    Total DRAM Elapsed Cycles        cycle   70,321,152
    Average L1 Active Cycles         cycle 2,287,159.70
    Total L1 Elapsed Cycles          cycle   68,745,910
    Average L2 Active Cycles         cycle   636,339.92
    Total L2 Elapsed Cycles          cycle   51,810,744
    Average SM Active Cycles         cycle 2,287,159.70
    Total SM Elapsed Cycles          cycle   68,745,910
    Average SMSP Active Cycles       cycle 2,279,663.58
    Total SMSP Elapsed Cycles        cycle  274,983,640
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.78%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 70.48% above the average, while the minimum instance value is 25.52% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst    9,401,966
    Branch Efficiency                   %        62.47
    Avg. Divergent Branches                  16,333.91
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 20.23%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 2554316 excessive sectors (69% of the     
          total 3721366 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 4, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.00
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle    1,835,483
    Memory Throughput                 %         8.50
    DRAM Throughput                   %         1.10
    Duration                         ms         1.34
    L1/TEX Cache Throughput           %        12.78
    L2 Cache Throughput               %         8.50
    SM Active Cycles              cycle 1,828,938.53
    Compute (SM) Throughput           %        10.81
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       786.43
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.84
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.84
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.43%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.71
    Mem Busy                    %         6.22
    Max Bandwidth               %         8.50
    L1/TEX Hit Rate             %        20.89
    L2 Hit Rate                 %        96.59
    Mem Pipes Busy              %         2.57
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.758%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.85
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.15
    Active Warps Per Scheduler          warp         7.01
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.15%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.01 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.59
    Warp Cycles Per Executed Instruction           cycle        66.31
    Avg. Active Threads Per Warp                                19.88
    Avg. Not Predicated Off Threads Per Warp                    18.28
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62.46%                                                                                          
          On average, each warp of this workload spends 40.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.5% of the total average of 64.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.634%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   193,078.92
    Executed Instructions                           inst   23,169,470
    Avg. Issued Instructions Per Scheduler          inst   198,214.37
    Issued Instructions                             inst   23,785,724
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       1,048,576
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               34.13
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.56
    Achieved Active Warps Per SM           warp        28.02
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.44%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   103,745.33
    Total DRAM Elapsed Cycles        cycle   56,474,624
    Average L1 Active Cycles         cycle 1,828,938.53
    Total L1 Elapsed Cycles          cycle   55,020,060
    Average L2 Active Cycles         cycle   517,500.79
    Total L2 Elapsed Cycles          cycle   41,546,256
    Average SM Active Cycles         cycle 1,828,938.53
    Total SM Elapsed Cycles          cycle   55,020,060
    Average SMSP Active Cycles       cycle 1,827,029.12
    Total SMSP Elapsed Cycles        cycle  220,080,240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.94%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 70.05% above the average, while the minimum instance value is 25.03% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst    7,524,823
    Branch Efficiency                   %        62.48
    Avg. Divergent Branches                  13,069.88
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 20.52%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 2046625 excessive sectors (69% of the     
          total 2980971 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 3, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle    1,377,797
    Memory Throughput                 %         8.49
    DRAM Throughput                   %         1.13
    Duration                         ms         1.01
    L1/TEX Cache Throughput           %        12.77
    L2 Cache Throughput               %         8.49
    SM Active Cycles              cycle 1,369,041.53
    Compute (SM) Throughput           %        10.80
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       786.43
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.85
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.85
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.42%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.80
    Mem Busy                    %         6.22
    Max Bandwidth               %         8.49
    L1/TEX Hit Rate             %        20.91
    L2 Hit Rate                 %        96.41
    Mem Pipes Busy              %         2.57
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.752%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.87
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.13
    Active Warps Per Scheduler          warp         7.05
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.13%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.05 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.87
    Warp Cycles Per Executed Instruction           cycle        66.60
    Avg. Active Threads Per Warp                                19.88
    Avg. Not Predicated Off Threads Per Warp                    18.28
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 61.94%                                                                                          
          On average, each warp of this workload spends 40.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.9% of the total average of 64.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.633%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   144,742.28
    Executed Instructions                           inst   17,369,074
    Avg. Issued Instructions Per Scheduler          inst   148,604.84
    Issued Instructions                             inst   17,832,581
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,072
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread         786,432
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               25.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.72
    Achieved Active Warps Per SM           warp        28.07
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.28%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    79,827.33
    Total DRAM Elapsed Cycles        cycle   42,316,800
    Average L1 Active Cycles         cycle 1,369,041.53
    Total L1 Elapsed Cycles          cycle   41,264,560
    Average L2 Active Cycles         cycle   382,314.54
    Total L2 Elapsed Cycles          cycle   31,176,528
    Average SM Active Cycles         cycle 1,369,041.53
    Total SM Elapsed Cycles          cycle   41,264,560
    Average SMSP Active Cycles       cycle 1,366,735.03
    Total SMSP Elapsed Cycles        cycle  165,058,240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.74%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 70.48% above the average, while the minimum instance value is 26.09% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst    5,640,904
    Branch Efficiency                   %        62.49
    Avg. Divergent Branches                   9,796.12
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 20.21%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1533647 excessive sectors (69% of the     
          total 2233933 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 2, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle      924,838
    Memory Throughput                 %         8.50
    DRAM Throughput                   %         0.93
    Duration                         us       677.12
    L1/TEX Cache Throughput           %        12.73
    L2 Cache Throughput               %         8.50
    SM Active Cycles              cycle   917,444.30
    Compute (SM) Throughput           %        10.74
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.84
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.84
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.43%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.13
    Mem Busy                    %         6.22
    Max Bandwidth               %         8.50
    L1/TEX Hit Rate             %        20.90
    L2 Hit Rate                 %        96.52
    Mem Pipes Busy              %         2.56
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.734%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.85
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.15
    Active Warps Per Scheduler          warp         7.01
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.15%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          7.01 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.63
    Warp Cycles Per Executed Instruction           cycle        66.36
    Avg. Active Threads Per Warp                                19.90
    Avg. Not Predicated Off Threads Per Warp                    18.29
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 61.83%                                                                                          
          On average, each warp of this workload spends 40.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.8% of the total average of 64.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.602%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    96,832.07
    Executed Instructions                           inst   11,619,848
    Avg. Issued Instructions Per Scheduler          inst    99,424.58
    Issued Instructions                             inst   11,930,950
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               17.07
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.50
    Achieved Active Warps Per SM           warp        28.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.5%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    44,206.67
    Total DRAM Elapsed Cycles        cycle   28,416,000
    Average L1 Active Cycles         cycle   917,444.30
    Total L1 Elapsed Cycles          cycle   27,767,550
    Average L2 Active Cycles         cycle   253,843.38
    Total L2 Elapsed Cycles          cycle   20,934,288
    Average SM Active Cycles         cycle   917,444.30
    Total SM Elapsed Cycles          cycle   27,767,550
    Average SMSP Active Cycles       cycle   916,422.27
    Total SMSP Elapsed Cycles        cycle  111,070,200
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.6%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 70.79% above the average, while the minimum instance value is 26.81% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst    3,774,177
    Branch Efficiency                   %        62.50
    Avg. Divergent Branches                   6,553.58
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.99%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1028235 excessive sectors (69% of the     
          total 1497213 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle      465,892
    Memory Throughput                 %         8.46
    DRAM Throughput                   %         0.93
    Duration                         us       341.47
    L1/TEX Cache Throughput           %        12.65
    L2 Cache Throughput               %         8.46
    SM Active Cycles              cycle   458,973.43
    Compute (SM) Throughput           %        10.68
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.42
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        10.82
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        10.82
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.45%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s         3.13
    Mem Busy                    %         6.22
    Max Bandwidth               %         8.46
    L1/TEX Hit Rate             %        20.90
    L2 Hit Rate                 %        96.51
    Mem Pipes Busy              %         2.54
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 5.698%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.83
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        89.17
    Active Warps Per Scheduler          warp         6.99
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.17%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 9.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.99 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.58
    Warp Cycles Per Executed Instruction           cycle        66.34
    Avg. Active Threads Per Warp                                19.93
    Avg. Not Predicated Off Threads Per Warp                    18.32
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 62%                                                                                             
          On average, each warp of this workload spends 40.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.0% of the total average of 64.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.569%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 19.9 threads being active per cycle. This is further reduced  
          to 18.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    48,334.63
    Executed Instructions                           inst    5,800,156
    Avg. Issued Instructions Per Scheduler          inst    49,650.66
    Issued Instructions                             inst    5,958,079
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  1,024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                8.53
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.41
    Achieved Active Warps Per SM           warp        27.97
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.59%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    22,237.33
    Total DRAM Elapsed Cycles        cycle   14,314,496
    Average L1 Active Cycles         cycle   458,973.43
    Total L1 Elapsed Cycles          cycle   13,941,660
    Average L2 Active Cycles         cycle   122,344.54
    Total L2 Elapsed Cycles          cycle   10,542,024
    Average SM Active Cycles         cycle   458,973.43
    Total SM Elapsed Cycles          cycle   13,941,660
    Average SMSP Active Cycles       cycle   458,511.85
    Total SMSP Elapsed Cycles        cycle   55,766,640
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.03%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 71.92% above the average, while the minimum instance value is 28.07% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.32
    Branch Instructions              inst    1,883,679
    Branch Efficiency                   %        62.53
    Avg. Divergent Branches                   3,268.74
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.14%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 514323 excessive sectors (69% of the      
          total 748491 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

