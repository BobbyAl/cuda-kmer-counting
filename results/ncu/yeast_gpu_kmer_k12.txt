Reading sequence from ../data/ncbi_dataset_saccharomyces_cerevisiae/ncbi_dataset/data/GCF_000146045.2/GCF_000146045.2_R64_genomic.fna...
Sequence length: 12158327 bases
==PROF== Connected to process 207748 (/home/bobby/Documents/GitHub/cuda-kmer-counting/src/gpu_kmer)
Processing sequence in chunks of 1000000 bases
==PROF== Profiling "count_kmers_kernel" - 0: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 1: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 2: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 3: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 4: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 5: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 6: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 7: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 8: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 9: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 10: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 11: 0%....50%....100% - 31 passes
==PROF== Profiling "count_kmers_kernel" - 12: 0%....50%....100% - 31 passes
Found 6193876 unique 12-mers
GPU time: 41.0116 seconds
Top 10 most frequent k-mers: 
ATATATATATAT: 259
ACGTCTTCACCA: 259
CGTCTTCACCAT: 258
CACGTCTTCACC: 254
GTCTTCACCATC: 253
TATATATATATA: 246
CGTACACGTCTT: 242
ACACGTCTTCAC: 242
TACACGTCTTCA: 242
ACGTACACGTCT: 242
==PROF== Disconnected from process 207748
[207748] gpu_kmer@127.0.0.1
  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 13, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.38
    Elapsed Cycles                cycle    19,765,305
    Memory Throughput                 %         19.50
    DRAM Throughput                   %         19.50
    Duration                         ms         14.33
    L1/TEX Cache Throughput           %          3.46
    L2 Cache Throughput               %          4.37
    SM Active Cycles              cycle 19,674,100.70
    Compute (SM) Throughput           %          8.27
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         6.29
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.30
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.30
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.9%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.47
    Mem Busy                    %         3.58
    Max Bandwidth               %        19.50
    L1/TEX Hit Rate             %        42.37
    L2 Hit Rate                 %         6.39
    Mem Pipes Busy              %         2.09
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.77%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.28
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.72
    Active Warps Per Scheduler          warp         6.87
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.5%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.87 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        82.91
    Warp Cycles Per Executed Instruction           cycle        85.18
    Avg. Active Threads Per Warp                                18.35
    Avg. Not Predicated Off Threads Per Warp                    16.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 72.6%                                                                                           
          On average, each warp of this workload spends 60.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 72.6% of the total average of 82.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.9%                                                                                            
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.4 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,589,526.38
    Executed Instructions                           inst  190,743,165
    Avg. Issued Instructions Per Scheduler          inst 1,633,100.57
    Issued Instructions                             inst  195,972,068
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 13,312
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       3,407,872
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              110.93
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.17
    Achieved Active Warps Per SM           warp        27.57
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.83%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle 19,541,360.67
    Total DRAM Elapsed Cycles        cycle   601,211,904
    Average L1 Active Cycles         cycle 19,674,100.70
    Total L1 Elapsed Cycles          cycle   592,325,000
    Average L2 Active Cycles         cycle 18,282,575.46
    Total L2 Elapsed Cycles          cycle   447,530,184
    Average SM Active Cycles         cycle 19,674,100.70
    Total SM Elapsed Cycles          cycle   592,325,000
    Average SMSP Active Cycles       cycle 19,716,978.68
    Total SMSP Elapsed Cycles        cycle 2,369,300,000
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   63,975,210
    Branch Efficiency                   %        61.71
    Avg. Divergent Branches                 114,988.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 66.74%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12240146 excessive sectors (68% of the    
          total 17980348 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 12, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          7.02
    SM Frequency                    Ghz          1.38
    Elapsed Cycles                cycle    19,488,169
    Memory Throughput                 %         19.34
    DRAM Throughput                   %         19.34
    Duration                         ms         14.13
    L1/TEX Cache Throughput           %          3.45
    L2 Cache Throughput               %          4.39
    SM Active Cycles              cycle 19,506,609.57
    Compute (SM) Throughput           %          8.27
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         6.29
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.26
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.26
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.93%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.12
    Mem Busy                    %         3.54
    Max Bandwidth               %        19.34
    L1/TEX Hit Rate             %        42.39
    L2 Hit Rate                 %         5.93
    Mem Pipes Busy              %         2.10
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.768%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.26
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.74
    Active Warps Per Scheduler          warp         6.86
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.66%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.86 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        82.97
    Warp Cycles Per Executed Instruction           cycle        85.25
    Avg. Active Threads Per Warp                                18.32
    Avg. Not Predicated Off Threads Per Warp                    16.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 73.12%                                                                                          
          On average, each warp of this workload spends 60.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 73.1% of the total average of 83.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.907%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,568,271.95
    Executed Instructions                           inst  188,192,634
    Avg. Issued Instructions Per Scheduler          inst 1,611,397.97
    Issued Instructions                             inst  193,367,756
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 12,288
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       3,145,728
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              102.40
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        85.96
    Achieved Active Warps Per SM           warp        27.51
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.04%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle 19,166,675.33
    Total DRAM Elapsed Cycles        cycle   594,709,504
    Average L1 Active Cycles         cycle 19,506,609.57
    Total L1 Elapsed Cycles          cycle   584,664,170
    Average L2 Active Cycles         cycle 18,129,212.46
    Total L2 Elapsed Cycles          cycle   441,271,776
    Average SM Active Cycles         cycle 19,506,609.57
    Total SM Elapsed Cycles          cycle   584,664,170
    Average SMSP Active Cycles       cycle 19,498,030.85
    Total SMSP Elapsed Cycles        cycle 2,338,656,680
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   63,159,923
    Branch Efficiency                   %        61.66
    Avg. Divergent Branches                 113,648.15
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 67.1%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 12064111 excessive sectors (68% of the    
          total 17726917 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 11, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.98
    SM Frequency                    Ghz          1.38
    Elapsed Cycles                cycle    17,964,592
    Memory Throughput                 %         19.55
    DRAM Throughput                   %         19.55
    Duration                         ms         13.02
    L1/TEX Cache Throughput           %          3.47
    L2 Cache Throughput               %          4.37
    SM Active Cycles              cycle 17,878,613.23
    Compute (SM) Throughput           %          8.30
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.15
    Dropped Samples                sample          175
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.27
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.27
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.93%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.48
    Mem Busy                    %         3.53
    Max Bandwidth               %        19.55
    L1/TEX Hit Rate             %        42.38
    L2 Hit Rate                 %         5.97
    Mem Pipes Busy              %         2.10
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.775%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.26
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.74
    Active Warps Per Scheduler          warp         6.88
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.45%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.88 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        83.26
    Warp Cycles Per Executed Instruction           cycle        85.55
    Avg. Active Threads Per Warp                                18.33
    Avg. Not Predicated Off Threads Per Warp                    16.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 73.22%                                                                                          
          On average, each warp of this workload spends 61.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 73.2% of the total average of 83.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.92%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,438,295.27
    Executed Instructions                           inst  172,595,432
    Avg. Issued Instructions Per Scheduler          inst 1,477,836.20
    Issued Instructions                             inst  177,340,344
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 11,264
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       2,883,584
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               93.87
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.09
    Achieved Active Warps Per SM           warp        27.55
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.91%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle 17,763,864.67
    Total DRAM Elapsed Cycles        cycle   545,198,080
    Average L1 Active Cycles         cycle 17,878,613.23
    Total L1 Elapsed Cycles          cycle   534,118,350
    Average L2 Active Cycles         cycle 16,620,767.17
    Total L2 Elapsed Cycles          cycle   406,729,104
    Average SM Active Cycles         cycle 17,878,613.23
    Total SM Elapsed Cycles          cycle   534,118,350
    Average SMSP Active Cycles       cycle 17,893,856.82
    Total SMSP Elapsed Cycles        cycle 2,136,473,400
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   57,925,559
    Branch Efficiency                   %        61.66
    Avg. Divergent Branches                 104,226.76
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 66.75%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 11069682 excessive sectors (68% of the    
          total 16263583 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 10, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.97
    SM Frequency                    Ghz          1.38
    Elapsed Cycles                cycle    16,341,176
    Memory Throughput                 %         19.56
    DRAM Throughput                   %         19.56
    Duration                         ms         11.85
    L1/TEX Cache Throughput           %          3.46
    L2 Cache Throughput               %          4.32
    SM Active Cycles              cycle 16,204,897.97
    Compute (SM) Throughput           %          8.29
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.15
    Dropped Samples                sample           98
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.29
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.29
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.91%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.46
    Mem Busy                    %         3.50
    Max Bandwidth               %        19.56
    L1/TEX Hit Rate             %        42.38
    L2 Hit Rate                 %         5.97
    Mem Pipes Busy              %         2.10
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.773%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.29
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.71
    Active Warps Per Scheduler          warp         6.93
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.93 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        83.56
    Warp Cycles Per Executed Instruction           cycle        85.85
    Avg. Active Threads Per Warp                                18.33
    Avg. Not Predicated Off Threads Per Warp                    16.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 72.34%                                                                                          
          On average, each warp of this workload spends 60.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 72.3% of the total average of 83.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.916%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,307,125.15
    Executed Instructions                           inst  156,855,018
    Avg. Issued Instructions Per Scheduler          inst 1,343,066.72
    Issued Instructions                             inst  161,168,006
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 10,240
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       2,621,440
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               85.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.43
    Achieved Active Warps Per SM           warp        27.66
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.57%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle 16,155,439.33
    Total DRAM Elapsed Cycles        cycle   495,625,216
    Average L1 Active Cycles         cycle 16,204,897.97
    Total L1 Elapsed Cycles          cycle   485,944,960
    Average L2 Active Cycles         cycle 15,051,614.54
    Total L2 Elapsed Cycles          cycle   369,956,568
    Average SM Active Cycles         cycle 16,204,897.97
    Total SM Elapsed Cycles          cycle   485,944,960
    Average SMSP Active Cycles       cycle 16,196,921.58
    Total SMSP Elapsed Cycles        cycle 1,943,779,840
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   52,642,757
    Branch Efficiency                   %        61.66
    Avg. Divergent Branches                  94,719.91
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 66.46%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 10058849 excessive sectors (68% of the    
          total 14778923 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 9, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.38
    Elapsed Cycles                cycle    14,664,710
    Memory Throughput                 %         19.56
    DRAM Throughput                   %         19.56
    Duration                         ms         10.63
    L1/TEX Cache Throughput           %          3.45
    L2 Cache Throughput               %          4.30
    SM Active Cycles              cycle 14,534,726.80
    Compute (SM) Throughput           %          8.26
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.15
    Dropped Samples                sample           17
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.31
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.9%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.61
    Mem Busy                    %         3.55
    Max Bandwidth               %        19.56
    L1/TEX Hit Rate             %        42.40
    L2 Hit Rate                 %         5.99
    Mem Pipes Busy              %         2.09
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.767%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.32
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.68
    Active Warps Per Scheduler          warp         6.90
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.90 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        83.01
    Warp Cycles Per Executed Instruction           cycle        85.30
    Avg. Active Threads Per Warp                                18.32
    Avg. Not Predicated Off Threads Per Warp                    16.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 72.46%                                                                                          
          On average, each warp of this workload spends 60.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 72.5% of the total average of 83.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.906%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,175,749.36
    Executed Instructions                           inst  141,089,923
    Avg. Issued Instructions Per Scheduler          inst    1,208,105
    Issued Instructions                             inst  144,972,600
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  9,216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       2,359,296
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               76.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.61
    Achieved Active Warps Per SM           warp        27.72
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.39%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle 14,532,532.67
    Total DRAM Elapsed Cycles        cycle   445,692,928
    Average L1 Active Cycles         cycle 14,534,726.80
    Total L1 Elapsed Cycles          cycle   438,529,830
    Average L2 Active Cycles         cycle 13,511,245.92
    Total L2 Elapsed Cycles          cycle   332,146,056
    Average SM Active Cycles         cycle 14,534,726.80
    Total SM Elapsed Cycles          cycle   438,529,830
    Average SMSP Active Cycles       cycle 14,524,590.93
    Total SMSP Elapsed Cycles        cycle 1,754,119,320
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   47,351,397
    Branch Efficiency                   %        61.66
    Avg. Divergent Branches                  85,210.22
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 66.43%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 9040713 excessive sectors (68% of the     
          total 13285836 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 8, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- -------------
    Metric Name             Metric Unit  Metric Value
    ----------------------- ----------- -------------
    DRAM Frequency                  Ghz          6.99
    SM Frequency                    Ghz          1.38
    Elapsed Cycles                cycle    12,964,136
    Memory Throughput                 %         19.42
    DRAM Throughput                   %         19.42
    Duration                         ms          9.40
    L1/TEX Cache Throughput           %          3.44
    L2 Cache Throughput               %          4.42
    SM Active Cycles              cycle 13,019,281.97
    Compute (SM) Throughput           %          8.24
    ----------------------- ----------- -------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.15
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.25
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.25
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.94%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.21
    Mem Busy                    %         3.57
    Max Bandwidth               %        19.42
    L1/TEX Hit Rate             %        42.39
    L2 Hit Rate                 %         6.27
    Mem Pipes Busy              %         2.09
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.761%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.27
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.73
    Active Warps Per Scheduler          warp         6.86
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.58%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.86 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        82.99
    Warp Cycles Per Executed Instruction           cycle        85.27
    Avg. Active Threads Per Warp                                18.32
    Avg. Not Predicated Off Threads Per Warp                    16.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 73.51%                                                                                          
          On average, each warp of this workload spends 61.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 73.5% of the total average of 83.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.893%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst 1,045,613.04
    Executed Instructions                           inst  125,473,565
    Avg. Issued Instructions Per Scheduler          inst 1,074,390.57
    Issued Instructions                             inst  128,926,869
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  8,192
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       2,097,152
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               68.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        85.42
    Achieved Active Warps Per SM           warp        27.33
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.58%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (85.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle    12,768,530
    Total DRAM Elapsed Cycles        cycle   394,435,584
    Average L1 Active Cycles         cycle 13,019,281.97
    Total L1 Elapsed Cycles          cycle   391,187,630
    Average L2 Active Cycles         cycle 12,092,941.75
    Total L2 Elapsed Cycles          cycle   293,542,272
    Average SM Active Cycles         cycle 13,019,281.97
    Total SM Elapsed Cycles          cycle   391,187,630
    Average SMSP Active Cycles       cycle 12,987,911.93
    Total SMSP Elapsed Cycles        cycle 1,564,750,520
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   42,110,866
    Branch Efficiency                   %        61.66
    Avg. Divergent Branches                  75,780.91
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 67.28%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 8041335 excessive sectors (68% of the     
          total 11816740 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 7, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.95
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle   11,446,457
    Memory Throughput                 %        19.71
    DRAM Throughput                   %        19.71
    Duration                         ms         8.30
    L1/TEX Cache Throughput           %         3.43
    L2 Cache Throughput               %         4.40
    SM Active Cycles              cycle   11,318,883
    Compute (SM) Throughput           %         8.23
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.15
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.31
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.9%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.77
    Mem Busy                    %         3.54
    Max Bandwidth               %        19.71
    L1/TEX Hit Rate             %        42.39
    L2 Hit Rate                 %         6.09
    Mem Pipes Busy              %         2.09
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.759%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.25
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.75
    Active Warps Per Scheduler          warp         6.88
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.29%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.88 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        83.37
    Warp Cycles Per Executed Instruction           cycle        85.67
    Avg. Active Threads Per Warp                                18.32
    Avg. Not Predicated Off Threads Per Warp                    16.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 72.46%                                                                                          
          On average, each warp of this workload spends 60.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 72.5% of the total average of 83.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.889%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   915,346.03
    Executed Instructions                           inst  109,841,523
    Avg. Issued Instructions Per Scheduler          inst   940,552.53
    Issued Instructions                             inst  112,866,303
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  7,168
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       1,835,008
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               59.73
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.60
    Achieved Active Warps Per SM           warp        27.71
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.4%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle 11,370,361.33
    Total DRAM Elapsed Cycles        cycle   346,127,360
    Average L1 Active Cycles         cycle    11,318,883
    Total L1 Elapsed Cycles          cycle   342,896,650
    Average L2 Active Cycles         cycle 10,629,736.12
    Total L2 Elapsed Cycles          cycle   259,311,072
    Average SM Active Cycles         cycle    11,318,883
    Total SM Elapsed Cycles          cycle   342,896,650
    Average SMSP Active Cycles       cycle 11,402,219.81
    Total SMSP Elapsed Cycles        cycle 1,371,586,600
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   36,865,312
    Branch Efficiency                   %        61.65
    Avg. Divergent Branches                  66,348.11
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 66.95%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 7038519 excessive sectors (68% of the     
          total 10343444 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 6, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    9,736,735
    Memory Throughput                 %        19.42
    DRAM Throughput                   %        19.42
    Duration                         ms         7.06
    L1/TEX Cache Throughput           %         3.43
    L2 Cache Throughput               %         4.45
    SM Active Cycles              cycle 9,753,044.93
    Compute (SM) Throughput           %         8.22
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.15
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.26
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.26
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.93%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.13
    Mem Busy                    %         3.52
    Max Bandwidth               %        19.42
    L1/TEX Hit Rate             %        42.41
    L2 Hit Rate                 %         6.10
    Mem Pipes Busy              %         2.08
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.755%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.25
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.75
    Active Warps Per Scheduler          warp         6.88
    Eligible Warps Per Scheduler        warp         0.11
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.58%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.88 active warps per scheduler, but only an average of 0.11 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        83.33
    Warp Cycles Per Executed Instruction           cycle        85.63
    Avg. Active Threads Per Warp                                18.30
    Avg. Not Predicated Off Threads Per Warp                    16.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 72.55%                                                                                          
          On average, each warp of this workload spends 60.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 72.5% of the total average of 83.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.889%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   783,987.93
    Executed Instructions                           inst   94,078,551
    Avg. Issued Instructions Per Scheduler          inst   805,603.55
    Issued Instructions                             inst   96,672,426
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  6,144
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       1,572,864
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               51.20
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        85.94
    Achieved Active Warps Per SM           warp        27.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.06%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (85.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle     9,576,238
    Total DRAM Elapsed Cycles        cycle   295,801,856
    Average L1 Active Cycles         cycle  9,753,044.93
    Total L1 Elapsed Cycles          cycle   294,014,240
    Average L2 Active Cycles         cycle  9,069,597.29
    Total L2 Elapsed Cycles          cycle   220,402,056
    Average SM Active Cycles         cycle  9,753,044.93
    Total SM Elapsed Cycles          cycle   294,014,240
    Average SMSP Active Cycles       cycle  9,762,471.19
    Total SMSP Elapsed Cycles        cycle 1,176,056,960
    -------------------------- ----------- -------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   31,574,153
    Branch Efficiency                   %        61.64
    Avg. Divergent Branches                  56,834.30
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 67.18%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 6020042 excessive sectors (68% of the     
          total 8850178 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 5, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    8,077,784
    Memory Throughput                 %        19.46
    DRAM Throughput                   %        19.46
    Duration                         ms         5.86
    L1/TEX Cache Throughput           %         3.46
    L2 Cache Throughput               %         4.48
    SM Active Cycles              cycle 8,130,408.70
    Compute (SM) Throughput           %         8.31
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         1.57
    Dropped Samples                sample           58
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.25
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.25
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.94%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.28
    Mem Busy                    %         3.59
    Max Bandwidth               %        19.46
    L1/TEX Hit Rate             %        42.43
    L2 Hit Rate                 %         6.10
    Mem Pipes Busy              %         2.11
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.774%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.34
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.66
    Active Warps Per Scheduler          warp         6.89
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.54%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.89 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        82.56
    Warp Cycles Per Executed Instruction           cycle        84.84
    Avg. Active Threads Per Warp                                18.29
    Avg. Not Predicated Off Threads Per Warp                    16.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 73.85%                                                                                          
          On average, each warp of this workload spends 61.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 73.9% of the total average of 82.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.936%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   652,499.97
    Executed Instructions                           inst   78,299,996
    Avg. Issued Instructions Per Scheduler          inst   670,514.45
    Issued Instructions                             inst   80,461,734
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  5,120
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       1,310,720
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               42.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        85.21
    Achieved Active Warps Per SM           warp        27.27
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.79%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (85.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle 7,963,081.33
    Total DRAM Elapsed Cycles        cycle  245,565,440
    Average L1 Active Cycles         cycle 8,130,408.70
    Total L1 Elapsed Cycles          cycle  241,932,000
    Average L2 Active Cycles         cycle    7,485,898
    Total L2 Elapsed Cycles          cycle  182,984,904
    Average SM Active Cycles         cycle 8,130,408.70
    Total SM Elapsed Cycles          cycle  241,932,000
    Average SMSP Active Cycles       cycle 8,040,658.33
    Total SMSP Elapsed Cycles        cycle  967,728,000
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   26,277,998
    Branch Efficiency                   %        61.64
    Avg. Divergent Branches                  47,307.32
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 66.76%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 5004232 excessive sectors (68% of the     
          total 7359305 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 4, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.06
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    6,494,274
    Memory Throughput                 %        19.19
    DRAM Throughput                   %        19.19
    Duration                         ms         4.71
    L1/TEX Cache Throughput           %         3.43
    L2 Cache Throughput               %         4.32
    SM Active Cycles              cycle 6,524,112.17
    Compute (SM) Throughput           %         8.24
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         1.57
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.23
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.23
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.95%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.04
    Mem Busy                    %         3.57
    Max Bandwidth               %        19.19
    L1/TEX Hit Rate             %        42.41
    L2 Hit Rate                 %         5.77
    Mem Pipes Busy              %         2.09
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.759%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.23
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.77
    Active Warps Per Scheduler          warp         6.86
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.81%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.86 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        83.34
    Warp Cycles Per Executed Instruction           cycle        85.64
    Avg. Active Threads Per Warp                                18.30
    Avg. Not Predicated Off Threads Per Warp                    16.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 72.5%                                                                                           
          On average, each warp of this workload spends 60.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 72.5% of the total average of 83.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.896%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      522,323
    Executed Instructions                           inst   62,678,760
    Avg. Issued Instructions Per Scheduler          inst   536,742.12
    Issued Instructions                             inst   64,409,055
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  4,096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread       1,048,576
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               34.13
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        84.62
    Achieved Active Warps Per SM           warp        27.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 15.38%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    6,378,404
    Total DRAM Elapsed Cycles        cycle  199,445,504
    Average L1 Active Cycles         cycle 6,524,112.17
    Total L1 Elapsed Cycles          cycle  195,496,540
    Average L2 Active Cycles         cycle 6,088,390.12
    Total L2 Elapsed Cycles          cycle  146,966,112
    Average SM Active Cycles         cycle 6,524,112.17
    Total SM Elapsed Cycles          cycle  195,496,540
    Average SMSP Active Cycles       cycle 6,520,037.44
    Total SMSP Elapsed Cycles        cycle  781,986,160
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   21,036,107
    Branch Efficiency                   %        61.64
    Avg. Divergent Branches                  37,867.25
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 67.63%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4010988 excessive sectors (68% of the     
          total 5896491 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 3, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.86
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    4,936,602
    Memory Throughput                 %        20.20
    DRAM Throughput                   %        20.20
    Duration                         ms         3.58
    L1/TEX Cache Throughput           %         3.47
    L2 Cache Throughput               %         4.25
    SM Active Cycles              cycle 4,824,600.63
    Compute (SM) Throughput           %         8.33
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         1.57
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.32
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.34
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.34
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.88%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        66.50
    Mem Busy                    %         3.69
    Max Bandwidth               %        20.20
    L1/TEX Hit Rate             %        42.42
    L2 Hit Rate                 %         6.32
    Mem Pipes Busy              %         2.11
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.779%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.36
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.64
    Active Warps Per Scheduler          warp         6.86
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 79.8%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.86 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        82.00
    Warp Cycles Per Executed Instruction           cycle        84.26
    Avg. Active Threads Per Warp                                18.30
    Avg. Not Predicated Off Threads Per Warp                    16.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 75.06%                                                                                          
          On average, each warp of this workload spends 61.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 75.1% of the total average of 82.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.943%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   391,514.07
    Executed Instructions                           inst   46,981,688
    Avg. Issued Instructions Per Scheduler          inst   402,335.14
    Issued Instructions                             inst   48,280,217
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  3,072
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread         786,432
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               25.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        85.42
    Achieved Active Warps Per SM           warp        27.34
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.58%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (85.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle 4,958,125.33
    Total DRAM Elapsed Cycles        cycle  147,270,656
    Average L1 Active Cycles         cycle 4,824,600.63
    Total L1 Elapsed Cycles          cycle  144,826,340
    Average L2 Active Cycles         cycle 4,485,380.04
    Total L2 Elapsed Cycles          cycle  111,850,080
    Average SM Active Cycles         cycle 4,824,600.63
    Total SM Elapsed Cycles          cycle  144,826,340
    Average SMSP Active Cycles       cycle 4,810,899.09
    Total SMSP Elapsed Cycles        cycle  579,305,360
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   15,767,800
    Branch Efficiency                   %        61.65
    Avg. Divergent Branches                  28,380.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 65.47%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 3006144 excessive sectors (68% of the     
          total 4419361 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 2, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.79
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    3,353,438
    Memory Throughput                 %        20.53
    DRAM Throughput                   %        20.53
    Duration                         ms         2.43
    L1/TEX Cache Throughput           %         3.48
    L2 Cache Throughput               %         4.21
    SM Active Cycles              cycle 3,213,372.30
    Compute (SM) Throughput           %         8.34
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       786.43
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.33
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.38
    Issued Ipc Active     inst/cycle         0.34
    SM Busy                        %         8.38
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.86%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        66.96
    Mem Busy                    %         3.67
    Max Bandwidth               %        20.53
    L1/TEX Hit Rate             %        42.39
    L2 Hit Rate                 %         6.45
    Mem Pipes Busy              %         2.11
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.783%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.16
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.84
    Active Warps Per Scheduler          warp         6.73
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 79.47%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 12.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.73 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        82.53
    Warp Cycles Per Executed Instruction           cycle        84.82
    Avg. Active Threads Per Warp                                18.33
    Avg. Not Predicated Off Threads Per Warp                    16.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 72.14%                                                                                          
          On average, each warp of this workload spends 59.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 72.1% of the total average of 82.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.937%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.3 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   262,115.90
    Executed Instructions                           inst   31,453,908
    Avg. Issued Instructions Per Scheduler          inst   269,361.27
    Issued Instructions                             inst   32,323,352
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  2,048
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread         524,288
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               17.07
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.55
    Achieved Active Warps Per SM           warp        27.70
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.45%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    3,390,960
    Total DRAM Elapsed Cycles        cycle   99,097,600
    Average L1 Active Cycles         cycle 3,213,372.30
    Total L1 Elapsed Cycles          cycle   96,938,010
    Average L2 Active Cycles         cycle 3,079,633.17
    Total L2 Elapsed Cycles          cycle   75,883,656
    Average SM Active Cycles         cycle 3,213,372.30
    Total SM Elapsed Cycles          cycle   96,938,010
    Average SMSP Active Cycles       cycle 3,302,811.43
    Total SMSP Elapsed Cycles        cycle  387,752,040
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst   10,557,073
    Branch Efficiency                   %        61.66
    Avg. Divergent Branches                  18,996.61
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 66.3%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 2018060 excessive sectors (68% of the     
          total 2964614 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  count_kmers_kernel(const char *, int, int, unsigned int *, int) (1024, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.00
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle    1,621,875
    Memory Throughput                 %        19.44
    DRAM Throughput                   %        19.44
    Duration                         ms         1.19
    L1/TEX Cache Throughput           %         3.49
    L2 Cache Throughput               %         4.36
    SM Active Cycles              cycle 1,589,383.97
    Compute (SM) Throughput           %         8.36
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       786.43
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.33
    Executed Ipc Elapsed  inst/cycle         0.33
    Issue Slots Busy               %         8.46
    Issued Ipc Active     inst/cycle         0.34
    SM Busy                        %         8.46
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.81%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        65.27
    Mem Busy                    %         3.82
    Max Bandwidth               %        19.44
    L1/TEX Hit Rate             %        42.37
    L2 Hit Rate                 %         6.24
    Mem Pipes Busy              %         2.12
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.788%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.52
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.48
    Active Warps Per Scheduler          warp         6.89
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.89 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        80.80
    Warp Cycles Per Executed Instruction           cycle        83.04
    Avg. Active Threads Per Warp                                18.36
    Avg. Not Predicated Off Threads Per Warp                    16.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 73.05%                                                                                          
          On average, each warp of this workload spends 59.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 73.1% of the total average of 80.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.94%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 18.4 threads being active per cycle. This is further reduced  
          to 16.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   130,842.97
    Executed Instructions                           inst   15,701,156
    Avg. Issued Instructions Per Scheduler          inst   134,479.75
    Issued Instructions                             inst   16,137,570
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  1,024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              30
    Stack Size                                                 1,024
    Threads                                   thread         262,144
    # TPCs                                                        15
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                8.53
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        85.59
    Achieved Active Warps Per SM           warp        27.39
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.41%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (85.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle 1,614,880.67
    Total DRAM Elapsed Cycles        cycle   49,853,440
    Average L1 Active Cycles         cycle 1,589,383.97
    Total L1 Elapsed Cycles          cycle   48,277,830
    Average L2 Active Cycles         cycle 1,486,431.96
    Total L2 Elapsed Cycles          cycle   36,649,728
    Average SM Active Cycles         cycle 1,589,383.97
    Total SM Elapsed Cycles          cycle   48,277,830
    Average SMSP Active Cycles       cycle 1,577,499.78
    Total SMSP Elapsed Cycles        cycle  193,111,320
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.34
    Branch Instructions              inst    5,269,953
    Branch Efficiency                   %        61.67
    Avg. Divergent Branches                   9,481.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 66.3%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 1009142 excessive sectors (68% of the     
          total 1481683 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

